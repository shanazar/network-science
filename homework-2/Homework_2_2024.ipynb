{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XavWpYn0zluE"
   },
   "source": [
    "<h1><center> <font color='black'>  Network Science (LTAT.02.011) - Homework - 02  </font></center></h1>\n",
    "<h2><center> <font color='black'> Echo Chambers and Text Representation in Network Science</font></center></h3>\n",
    "<h2><center> <font color='black'> University of Tartu - 2024</font></center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9lyxS3Czpz_"
   },
   "source": [
    "================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjNRTGhjzq98"
   },
   "source": [
    "# Homework instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-M3Hm7Eiztkg"
   },
   "source": [
    "- This homework should be done individually or in group of 2 (max).\n",
    "\n",
    "- Please provide the names and student IDs of the team-members in the field \"Student info\" below. If you are not working in a team please insert only your name and student ID.\n",
    "\n",
    "- Only one of the teammates should submit the homework. We will grade the homework and the marks and feedback is applied for both the team members. So please communicate with your team member about marks and feedback if you are submitting the homework.\n",
    "\n",
    "- The accepted submission format is .ipynb file. Please upload the file on moodle.\n",
    "\n",
    "- The submission will automatically close on **<font color='red'>17 April 2024 at 23:59 pm</font>**, so please make sure you have enough time to submit the homework.\n",
    "\n",
    "- You do not necessarily need to work on Colab. Especially as the size and the complexity of datasets will increase through the course, you can install jupyter notebooks locally and work from there.\n",
    "\n",
    "- If you do not understand what a question is asking for, please ask in Moodle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6qIpqH3zuvH"
   },
   "source": [
    "**<h2><font color='red'>Student info:</font></h2>**\n",
    "\n",
    "\n",
    "<font color='red'>Full name: </font> Kristjan LÃµhmus\n",
    "\n",
    "<font color='red'>Student ID: </font>B65854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyJW2iNp0QiI"
   },
   "source": [
    "# Installing and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aK2JVxNMzRBX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bdb8d868-5e1d-4be5-8ae2-3d5a269e143f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (1.24.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (4.39.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.3)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.8)\n",
      "Requirement already satisfied: sympy in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sumy in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: pycountry>=18.2.23 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sumy) (23.12.11)\n",
      "Requirement already satisfied: nltk>=3.0.2 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sumy) (3.8.1)\n",
      "Requirement already satisfied: docopt<0.7,>=0.6.1 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sumy) (0.6.2)\n",
      "Requirement already satisfied: breadability>=0.1.20 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sumy) (0.1.20)\n",
      "Requirement already satisfied: requests>=2.7.0 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sumy) (2.31.0)\n",
      "Requirement already satisfied: chardet in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
      "Requirement already satisfied: lxml>=2.0 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk>=3.0.2->sumy) (4.66.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk>=3.0.2->sumy) (2023.12.25)\n",
      "Requirement already satisfied: joblib in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk>=3.0.2->sumy) (1.3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.7.0->sumy) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.7.0->sumy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.7.0->sumy) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk>=3.0.2->sumy) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pandas in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\krist\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Installing additional dependencies\n",
    "!pip install sentence-transformers\n",
    "!pip install sumy\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kqkXtA7j0c-V",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a13d2ef9-b61e-4cd0-bfdd-db659cafda34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\krist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from community import community_louvain\n",
    "import matplotlib.cm as cm\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import load_dataset\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. News semantic matching and summarization\n",
    "\n",
    "You are a data analyst for a media publishing company. The media publishing company keen on gaining insights from competitors by identifying news events covered by various publishing houses.\n",
    "\n",
    "In addition, the media publishing company wishes that one news event is then summarized from the most important sentences. The company can use this information to better write their own report on the news event. The media publishing company has provided you with their collected news articles from competitors pages (ag_news dataset)."
   ],
   "metadata": {
    "id": "ZHf4y6oFav0f"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clccQcuw6O3W"
   },
   "source": [
    "## 1.1 Reading Data and Preprocessing (5 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9n0yLkp0mo5"
   },
   "source": [
    "Before working with the data we need to preprocess the data to remove unhelpful noise.\n",
    "\n",
    "Let us first read in the data. This has been done for you."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"ag_news\")\n",
    "data = pd.DataFrame(dataset['train'])\n",
    "\n",
    "data = data.rename(columns={'text': 'Article'})\n",
    "\n",
    "data.head()"
   ],
   "metadata": {
    "id": "vPDDqqpk6Lf6"
   },
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/8.07k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "edf54d2e8db34cbc8b51f5b24f4956b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|ââââââââââ| 18.6M/18.6M [00:01<00:00, 13.0MB/s]\n",
      "Downloading data: 100%|ââââââââââ| 1.23M/1.23M [00:00<00:00, 3.51MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e369a0917d84fd483d06129abd31f8f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1d51b1667ed4be6bff2d84d9508c559"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                                             Article  label\n0  Wall St. Bears Claw Back Into the Black (Reute...      2\n1  Carlyle Looks Toward Commercial Aerospace (Reu...      2\n2  Oil and Economy Cloud Stocks' Outlook (Reuters...      2\n3  Iraq Halts Oil Exports from Main Southern Pipe...      2\n4  Oil prices soar to all-time record, posing new...      2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Article</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Oil prices soar to all-time record, posing new...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LLCt1VXS5bBW"
   },
   "outputs": [],
   "source": [
    "# Lets drop columns we do not need\n",
    "\n",
    "news = data[[\"Article\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4w9mLI7R53kS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimension: (120000, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Dataset dimension:', news.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rFcy8J5448Mc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                             Article\n0  Wall St. Bears Claw Back Into the Black (Reute...\n1  Carlyle Looks Toward Commercial Aerospace (Reu...\n2  Oil and Economy Cloud Stocks' Outlook (Reuters...\n3  Iraq Halts Oil Exports from Main Southern Pipe...\n4  Oil prices soar to all-time record, posing new...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Article</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Oil prices soar to all-time record, posing new...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "news['Article'] = news['Article'].apply(str)\n",
    "news['Article_length'] = news['Article'].apply(len)\n",
    "\n",
    "print(f\"Maximum article length: {news['Article_length'].max()}\")\n",
    "print(f\"Minimum article length: {news['Article_length'].min()}\")"
   ],
   "metadata": {
    "id": "65W728W6Z8MJ"
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum article length: 1012\n",
      "Minimum article length: 100\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGcg5aLS7XqF"
   },
   "source": [
    "**1.1.1. Semantic similarity calculation works best if we have texts that are relatively similar in length. From the cell above we can see that some articles are too short or too long to be accurately matched. Let us include into our dataset only articles, that are longer than 150 chars and shorter than 200 chars.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aRcfjVej75IQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum article length in filtered dataset: 199\n",
      "Minimum article length in filtered dataset: 151\n",
      "Dataset dimension: (25046, 2)\n"
     ]
    }
   ],
   "source": [
    "filtered_news = news[(news['Article_length'] < 200) & (news['Article_length'] > 150)]\n",
    "# Use this to check that your code did the required change\n",
    "print(f\"Maximum article length in filtered dataset: {filtered_news['Article_length'].max()}\")\n",
    "print(f\"Minimum article length in filtered dataset: {filtered_news['Article_length'].min()}\")\n",
    "print('Dataset dimension:', filtered_news.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d-Wii4p808z"
   },
   "source": [
    "**1.1.2. Create a new column for your preprocessed article text (we want to keep the original so that we can do summarization later).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "e_uWgHAx76y4"
   },
   "outputs": [],
   "source": [
    "filtered_news[\"Preprocessed_article\"] = filtered_news['Article']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9YbALDk9P6L"
   },
   "source": [
    "**1.1.3. Lowercase the articles and count how many lines were changed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aYfpiHBx9N2W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of articles that include some uppercase characters: 25046\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                              Article  Article_length  \\\n18  US trade deficit swells in June The US trade d...             155   \n19  Shell 'could be target for Total' Oil giant Sh...             155   \n20  Google IPO faces Playboy slip-up The bidding g...             171   \n21  Eurozone economy keeps growing Official figure...             161   \n23  Rand falls on shock SA rate cut Interest rates...             162   \n\n                                 Preprocessed_article  \n18  us trade deficit swells in june the us trade d...  \n19  shell 'could be target for total' oil giant sh...  \n20  google ipo faces playboy slip-up the bidding g...  \n21  eurozone economy keeps growing official figure...  \n23  rand falls on shock sa rate cut interest rates...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Article</th>\n      <th>Article_length</th>\n      <th>Preprocessed_article</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18</th>\n      <td>US trade deficit swells in June The US trade d...</td>\n      <td>155</td>\n      <td>us trade deficit swells in june the us trade d...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Shell 'could be target for Total' Oil giant Sh...</td>\n      <td>155</td>\n      <td>shell 'could be target for total' oil giant sh...</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Google IPO faces Playboy slip-up The bidding g...</td>\n      <td>171</td>\n      <td>google ipo faces playboy slip-up the bidding g...</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Eurozone economy keeps growing Official figure...</td>\n      <td>161</td>\n      <td>eurozone economy keeps growing official figure...</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Rand falls on shock SA rate cut Interest rates...</td>\n      <td>162</td>\n      <td>rand falls on shock sa rate cut interest rates...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_news[\"Preprocessed_article\"] = filtered_news[\"Preprocessed_article\"].str.lower()\n",
    "count_of_articles_with_uppercase_letters = sum(filtered_news[\"Article\"] != filtered_news[\"Preprocessed_article\"])\n",
    "print(f\"Count of articles that include some uppercase characters: {count_of_articles_with_uppercase_letters}\")\n",
    "filtered_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVIT7ThmBcMC"
   },
   "source": [
    "**1.1.4. Remove all urls from the articles and count how many lines were changed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "oa1cO_lG9oiS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of articles that include urls: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                              Article  Article_length  \\\n18  US trade deficit swells in June The US trade d...             155   \n19  Shell 'could be target for Total' Oil giant Sh...             155   \n20  Google IPO faces Playboy slip-up The bidding g...             171   \n21  Eurozone economy keeps growing Official figure...             161   \n23  Rand falls on shock SA rate cut Interest rates...             162   \n\n                                 Preprocessed_article  \n18  us trade deficit swells in june the us trade d...  \n19  shell 'could be target for total' oil giant sh...  \n20  google ipo faces playboy slip-up the bidding g...  \n21  eurozone economy keeps growing official figure...  \n23  rand falls on shock sa rate cut interest rates...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Article</th>\n      <th>Article_length</th>\n      <th>Preprocessed_article</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18</th>\n      <td>US trade deficit swells in June The US trade d...</td>\n      <td>155</td>\n      <td>us trade deficit swells in june the us trade d...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Shell 'could be target for Total' Oil giant Sh...</td>\n      <td>155</td>\n      <td>shell 'could be target for total' oil giant sh...</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Google IPO faces Playboy slip-up The bidding g...</td>\n      <td>171</td>\n      <td>google ipo faces playboy slip-up the bidding g...</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Eurozone economy keeps growing Official figure...</td>\n      <td>161</td>\n      <td>eurozone economy keeps growing official figure...</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Rand falls on shock SA rate cut Interest rates...</td>\n      <td>162</td>\n      <td>rand falls on shock sa rate cut interest rates...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'(https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&/=]*)'\n",
    "count_of_articles_with_urls = filtered_news[\"Preprocessed_article\"].str.count(pattern).sum()\n",
    "filtered_news[\"Preprocessed_article\"] = filtered_news[\"Preprocessed_article\"].str.replace(pattern, '')\n",
    "\n",
    "print(f\"Count of articles that include urls: {count_of_articles_with_urls}\")\n",
    "filtered_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBgHQpO9CIu6"
   },
   "source": [
    "**1.1.5. Add one other pre-processing step of your choice and print the count of rows affected. Justify your choice for the pre-processing step.**\n",
    "\n",
    "<font color='red'>How you should answer the following questions. First, in the `#TODO` cell you have to write the code representing your solution. And in the `Your answer` cell write the answer in your own words.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "OtGXOPte_KmN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of modified rows 0\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "modified_articles = filtered_news[\"Preprocessed_article\"].apply(remove_stopwords)\n",
    "print(f'Number of modified rows {sum(modified_articles != filtered_news[\"Preprocessed_article\"])}')\n",
    "filtered_news[\"Preprocessed_article\"] = modified_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH3EOH9jDCaV"
   },
   "source": [
    "**<font color='red'>Your answer:</font>** Stopwords don't add much to the text, so removing them should make the analysis easier."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will also reset the index to make working in exercise 2 a bit easier."
   ],
   "metadata": {
    "id": "4EAOYyv8qAha"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "filtered_news.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "id": "56tT6a9xY9Dn"
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1P77s2KUCzzg"
   },
   "source": [
    "## 1.2. Similarity calculation and graph creation (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WP803a1MHdVR"
   },
   "source": [
    "**1.2.1. Load in a Sentence Transformers model (this is already given)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "bqAuHkHRIw_O"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5091edd460ba401ebea4e0bb2f974fbc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\krist\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4594816cf7f74f2b93bad3e6d898da5f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91ba7498de2b481e9f6d1e63a1cdda7d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ea659398fa54ac4a39893d06c0f52f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "255eea3451414550a24707002a0ba678"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d845e6db59a64b30851b60fcd0a2460c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d485f7cc15824a5f9b218271d7a42016"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cbcacac45a04071a376ce10b1d9cda7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5054ff0d071444ba835a2c9b966ee386"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6f857442a734b9587ec55289d646f87"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7380239c0e7c4ea2852455849ee49399"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amw0h7k-I4BN"
   },
   "source": [
    "**1.2.2. Calculate the embeddings on our pre-processed articles. Note this might take a long time (less than 10 minutes is expected though).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "d20uNZieJDQQ"
   },
   "outputs": [],
   "source": [
    "embeddings = np.array(model.encode(filtered_news[\"Preprocessed_article\"].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGeScQ_PJCa1"
   },
   "source": [
    "**1.2.3. Calculate the cosine similarity between the embeddings as a matrix (we recommend using the util.cos_sim function as it is faster than some others)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "kmrnyRd4J3lj"
   },
   "outputs": [],
   "source": [
    "cosine_sim_matrix = util.cos_sim(embeddings, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvoB3zwIKBsx"
   },
   "source": [
    "**1.2.4. Build the graph. Please use a similarity threshold of 0.85.**\n",
    "\n",
    "Hint: Use numpy vectorized operations, otherwise this could take long (but its also fine if you use a simple for-loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LdMMG66KIqK"
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "threshold = 0.85\n",
    "\n",
    "for i in range(cosine_sim_matrix.shape[0]):\n",
    "    for j in range(i + 1, cosine_sim_matrix.shape[1]):\n",
    "        if i not in G:\n",
    "            G.add_node(i, text=data.loc[i, 'tweet'])\n",
    "        if j not in G:\n",
    "            G.add_node(j, text=data.loc[j, 'tweet'])\n",
    "        if cosine_sim_matrix[i, j] > threshold:\n",
    "            G.add_edge(i, j, weight=float(cosine_sim_matrix[i, j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PC9K_ZAPKeWu"
   },
   "source": [
    "**1.2.6. Plot the top 10 largest (with the most nodes) network components.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tL3QeYOPKyPV"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXzCuUgbKyjB"
   },
   "source": [
    "**1.2.7. Print the articles of one component of your choice. Are these articles similar, meaning do they cover the same news event? Why/Why not?**\n",
    "\n",
    "<font color='red'>How you should answer the following questions. First, in the `#TODO` cell you have to write the code representing your solution. And in the `Your answer` cell write the answer in your own words.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-M5wOVRKLCHa"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5T1aWgbJ28r"
   },
   "source": [
    "**<font color='red'>Your answer:</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3. Summarization (5 points)"
   ],
   "metadata": {
    "id": "13zpcipRjc9r"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.3.1. Please extract all of the articles of one component of your choice.**"
   ],
   "metadata": {
    "id": "1YFiNK5QsDBF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "articles = #TODO\n"
   ],
   "metadata": {
    "id": "iFcayLrVj_qC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.3.2. Now use LexRank to summarize those sentences. Your summary should compose of three sentences. Is your summary good? Why/Why not?**\n",
    "\n",
    "<font color='red'>How you should answer the following questions. First, in the `#TODO` cell you have to write the code representing your solution. And in the `Your answer` cell write the answer in your own words.</font>"
   ],
   "metadata": {
    "id": "UAIYHFZ5sKZi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#TODO"
   ],
   "metadata": {
    "id": "nrDRxjDWoQ_8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**<font color='red'>Your answer:</font>**"
   ],
   "metadata": {
    "id": "Pt3Es08YvIhF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Homophily detection (5 points)\n",
    "\n",
    "Your next task is to put yourself in the role of a data analyst of a book shop. This book shop sells books on American politics. The bookshop owner wishes to undestand if peoples political leaning influences which books they purchase. For you to analyse this, he has already categorized all of the books by political leaning (conservative, neutral and liberal) and marked down which books are copurchased frequently. One way we can analyse if political leaning plays a role in copurchasing patterns, is by looking at the homophily of the network. Luckily for you the bookshop owner has already made this data into a nice graph (polbooks dataset)."
   ],
   "metadata": {
    "id": "zlwu_SdaRB62"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us first load in the prepared dataset."
   ],
   "metadata": {
    "id": "nwPBNQgLfFeO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color_map_polbooks = {'c': 'red', 'n': 'gray', 'l': 'blue'}\n",
    "G_polbooks = nx.read_graphml('polbooks.xml')\n",
    "node_colors_polbooks = [color_map_polbooks.get(G_polbooks.nodes[node]['value'], 'grey') for node in G_polbooks.nodes()]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw_networkx(G_polbooks, node_size=10, with_labels=False, node_color=node_colors_polbooks, edge_color='gray')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "s9sC0IfjRQjH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.1. Calculate the E-I Index of the network. What can you say from the index score you get regarding homophily?**\n",
    "\n",
    "<font color='red'>How you should answer the following questions. First, in the `#TODO` cell you have to write the code representing your solution. And in the `Your answer` cell write the answer in your own words.</font>"
   ],
   "metadata": {
    "id": "va-4NnQJPHo2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#TODO"
   ],
   "metadata": {
    "id": "GCM8SlDMPGAd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**<font color='red'>Your answer:</font>**"
   ],
   "metadata": {
    "id": "ZDAhjygsP5RC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.2. Calculate the modularity of the network. What can you say from the modularity score you get regarding homophily?**\n",
    "\n",
    "<font color='red'>How you should answer the following questions. First, in the `#TODO` cell you have to write the code representing your solution. And in the `Your answer` cell write the answer in your own words.</font>"
   ],
   "metadata": {
    "id": "m8WslZwKPbGP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#TODO"
   ],
   "metadata": {
    "id": "R223bceaPiBJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**<font color='red'>Your answer:</font>**"
   ],
   "metadata": {
    "id": "xeB0Fqz3P6RE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.3. Calculate the Random Walk Controversy of the network. What can you say from the RWC score you get regarding homophily?**\n",
    "\n",
    "<font color='red'>How you should answer the following questions. First, in the `#TODO` cell you have to write the code representing your solution. And in the `Your answer` cell write the answer in your own words.</font>"
   ],
   "metadata": {
    "id": "-_8QYxNrPigX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#TODO"
   ],
   "metadata": {
    "id": "ueXAhHi0Pq0C"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**<font color='red'>Your answer:</font>**"
   ],
   "metadata": {
    "id": "C7OmJGLYP7Dw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.4. What conclusions can you make to the book shop owner based on your metrics? Is there are strong tendency to copurchase books from the same political leaning?**"
   ],
   "metadata": {
    "id": "2_AGc7Q9KeQb"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7q6uM-hNIwXN"
   },
   "source": [
    "# How long did it take you to solve the homework?\n",
    "\n",
    "* Please answer as precisely as you can. It does not affect your points or grade in any way. It is okay, if it took 0.5 hours or 24 hours. The collected information will be used to improve future homeworks. Please change X into your estimate.\n",
    "\n",
    "<font color='red'> **Answer:**</font> X hours\n",
    "\n",
    "# What is the level of difficulty for this homework?\n",
    "Please put a number between $0:10$ ($0:$ easy, $10:$ difficult)\n",
    "\n",
    "<font color='red'> **Answer:**</font>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "iyJW2iNp0QiI",
    "a9n0yLkp0mo5",
    "clccQcuw6O3W",
    "1P77s2KUCzzg",
    "13zpcipRjc9r"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
